# SLURM - заметки

# Docker

Хорошая практика - запускать процессы внутри контейнера в foreground (при этом сам запущенный контейнер может работать и в background)
Слои в образе - readonly
.dockerignore - указывать директории, которые не надо класть в образ
Отключение кэшей - для пакетных менеджеров и для сборщиков типа pip
Например для APT внутри директивы установки пакетов - добавить

`RUN apt-get update && apt-get install ... && rm -rf /var/lib/apt/lists/*`

Концепция контейнеров не предполагает заходить внутрь контейнера для сбора и прочтения логов - логи пишутся в stdout/stderr
Все что контейнер написал в stdout/stderr - docker переведет в JSON, добавит метаинформацию и сложит на хосте в определенную директорию
Для того чтобы ловить логи nginx из стандартных файлов куда он их складывает - можно добавить директиву

`RUN set -ex && \
	ln -sf /dev/stdout /var/log/nginx/access.log && \
	ln -sf /dev/stderr /var/log/nginx/error.log`

set -ex - это чтобы видеть, какие команды в какой момент выполняются
Разница между COPY и ADD - ADD может скопировать файлы из удаленного источника, разархивировать архив и так далее. Если надо просто скопировать файл с хоста в контейнер - проще использовать COPY.
Разница между ENTRYPOINT и CMD - последняя чаще всего используется так же как ENTRYPOINT, но есть нюансы
ENTRYPOINT - должен использоваться при старте контейнера
CMD - добавлять параметры для энтрипойнта
То есть запуск приложения - через ENTRYPOINT, а ключи для запуска передать в CMD

Docker не про контейнеры. Docker про стандартизацию поставки и распространению ПО.
Воспроизводимость, консистентность.

Docker engine - это демон
Запущен как обычный процесс
docker build, ps, run - консольные утилиты, передают GRPC-команды на докер-демон
На том же хосте утилита обращается в UNIX-сокет локальной машины

Docker registry - хранилище images

## Namespace Isolation

Namespace - механизм изоляции процессов друг от друга, вшит в ядро Linux - PID, Net, Mount, User
  - PID NS - пространство имен айди процессов. PID 1 - это корневой процесс, без него работа системы завершается. Если скрипт внутри контейнера конечный - контейнер завершается (по умолчанию запуск под PID 1).
  - NET NS - сетевое окружение: сетевые интерфейсы (+lo), route tables, iptables, sockets
  - в один NS можно добавлять несколько процессов (например несколько контейнеров объединить общим localhost через который контейнеры могут общаться)
  - MNT NS - своя root fs, свои приватные маунты
  - правила сетевого NS можно тюнить в части параметров sysctl для каждого контейнера отдельно
  - USER NS - позволяет мапить GID/UID с хоста

Докер это в первую очередь система упаковки и доставки, а не обеспечения безопасности и защиты контейнеров.
Докер не обеспечивает суперизоляцию процессов и их безопасность, это скорее про ВМ или физику.

CGroups - механизм изоляции вычислительных ресурсов процессов друг от друга - CPU, Memory, I/O, Net
  - добавляют overhead - или нет. В зависимости от ситуации

Copy-On-Write - тоже механика ядра Linux. При чтении используется общая копия, при записи - новая копия. Используется также в LXC

## Storage-драйверы

  - самые популярные - overlay, devicemapper, AUFS
  - devicemapper - в CentOS дефолтный до недавнего времени, работает через loopback, надо использовать в версии с SYN-пулами. 
  - overlay - хорошая поддержка лишь в ядре 4.X. Поддерживается в ядре от 3.10 - CentOS 7.4. Для 7.4 рекомендуется overlay2.
  - для Ubuntu и Debian - лучший драйвер это AUFS.
  - BTRFS/ZFS - первый не развивается, второй под Linux практически тоже. 

## Log-драйверы.

  - по умолчанию - JSON-файл, со своей задачей справляется.
  - journald - из системного журнала. Нужно тюнить, лимиты по умолчанию на прием логов скромные. Плюс к тому есть хард-лимиты - выше определенных значений его не затюнить и поэтому лучше его не использовать.
  - syslog - по UDP/TCP/TLS
  - fluentd - по TCP, можно собирать логи из всех источников одновременно
  - ETW -> Windows Event Store
  - Splunk
  - Geif
  - Logentries
  - Amazon CloudWatch
  - Google Cloud
  - из драйвера логи попадают в log shipper.
  - рекомендуется для Docker остаться на JSON-файле (не теряя возможности выполнять docker logs), собирать логи через fluentd, потом в ElasticSearch + Kibana для просмотра.

Полезные опции
  - dockerd --default-ulimit - аналог docker run -ulimit, переопределяет максимальное число процессов, которые может запускать контейнер.
  - --dns - переопределять DNS-серверы
  - --insecure-registry - если свой реестр без сертификата, по умолчанию docker не даст подключиться к реестру без TLS.

## Docker Compose

Это обертка на Python над Docker - позволяет по описанию в YAML запускать многоконтейнерную структуру.

В 3 версии файла (версия декларируется параметром version: в YAML) выпилили поддержку healthcheck-ов и зависимостей.

healthcheck:
  test: ["CMD", "команда", "параметры"]
  interval: 1s
  timeout: 1s
  retries: 60

Healthcheck - это команда, которая должна возвращать нулевой exit-код. Если не возвращает - повторяется с нужным интервалом, после нужного таймаута и нужное количество раз.

docker-compose.override.yml - применяется автоматически при docker-compose up без указания дополнительных файлов.
Как пример - можно описать пробрасываемые порты и подключаемые тома с локального хоста при локальной разработке.
docker-compose.test.yml - нужно руками указывать docker-compose up -f <файл>. Оверрайд при этом отключается.
Как пример - использование переменных для тестового окружения.

## Docker registry

Альтернативные продукты - Harbor
  - хранение образов
  - авторизация
  - ротация
  - сканирование 
  - hook API
  - хранение других сущностей

Можно указать альтернативный registry в docker run, по умолчанию образы берутся с hub.docker.io

## Лучшие практики, паттерны и антипаттерны

Файлы, где описаны зависимости - копируются в начале
Логично что после копирования файлов - надо делать установку зависимостей, а потом копировать сам код.
Если копировать скрипты например из Debian на Alpine - то надо следить как именуются пользователи. В Debian nginx запускается под пользователем www-data, в Alpine - под nginx.
Добавляя пользователя внутрь контейнера - усложняется процесс сборки, если можно без этого обойтись - то надо обходиться.

Update делать нормально, upgrade - нет, так как это плохое, непредсказуемое поведение. При каждой сборке образа будет апгрейд пакетов, и что-то может поломаться.

Dev-пакеты для того чтобы собрать зависимости - лучше не делать в образе, в котором будет запускаться приложение. Лучше их собирать в новый базовый образ.

Надо отключать кэши и не тащить в образ.

Пакеты надо ставить до добавления кода, если так не делать - любое изменение кода будет вызывать повторную выкачку и установку пакетов.

Бандлы пакетов лучше выносить в переменные и указывать в начале Dockerfile, чтобы можно было потом легко вносить изменения:

`ARG BUILD_PACKAGES="nginx nodejs dcron tzdata postgresql-dev libxslt-dev"`

Статику можно собирать либо в Dockerfile, либо в ENTRYPOINT.
Лучше в Dockerfile, так как контейнеры могут падать, рестартоваться, а смысл контейнеров - как можно быстрее поднять работающее приложение.

В идеале 1 контейнер = 1 процесс. В реальной жизни часто приходится искать компромиссы. 

Приложения надо адаптировать. Если приложение не писалось под Docker - есть ненулевая вероятность, что оно не будет нормально работать. Пример - Java 7, которая не могла правильно определять количество процессоров внутри контейнера.

Архитектуру надо прорабатывать - как с точки зрения инфраструктуры, так и с точки зрения приложений и взаимодействия между ними. Как пример - делить приложение на фронт и бэк с общением между ними по HTTP.

Docker надо мониторить, причем не так как сервера. Контейнеры - штука непостоянная. Нужен autodiscovery, чтобы следить за новыми контейнерами. Нужно мониторить сами сервисы - следить за доступностью сервиса, а не конкретного контейнера. 

Чтобы успешно управлять контейнерной инфраструктурой - надо много дополнительных сущностей:
  - Nomad
  - Docker Swarm
  - Kubernetes
  - мониторинг
  - логирование


# Ansible

Плюсы:
  - не нужен агент
  - работа по SSH
  - не нужно программировать - вся настройка в YAML
  - идемпонентность - описывается состояние в которое должна быть приведена система и в идеале данное состояние должно быть идентичным для разных запусков в разное время для системы.
  - нужен только Python

Минусы:
  - нет агента - задачи загружаются по одной и в том порядке что указал создатель плейбука
  - нужен Python
  - YAML - нельзя делать сложную логику и надо следить за пробелами и отступами

Shell-скриптами можно заменить какие-то простые задачи, но в конечном итоге можно увлечься и написать свой Ansible, в котором будут проблемы:
  - время на изобретение велосипеда
  - затраты сотрудников на изучение этого велосипеда

Установить можно через pip (pip install ansible)

`ansible -m setup <host>` - сбор фактов с сервера

`ansible -m ping -i red all -u bond -k`
  - -m - модуль
  - -i - инвентори
  - all - все хосты во всех группах в инвентори
  - -u - пользователь
  - -k - запросить пароль

## Сущности Ansible

Playbook
ключи:
  - --diff - показать какие изменения сделал таск (что было и что стало)
  - --force-handlers - выполнять хендлеры независимо от того, завершился сценарий успешно или неуспешно. По умолчанию сценарий останавливается на ошибке.
  - --inventory - указатель на инвентарь
  - --limit - выполнять задачи не на всех серверах а на определенных группах или определенных хостах
  - --step - используется редко, пошаговое выполнение тасков
  - --become - сделать sudo от имени пользователя, повышение привилегий, можно написать и в инвентаре, и в таске, и в плейбуке
  - --ask-pass - короткая форма -k, запрос на пароль при подключении по ssh

Inventory

Group

## Ускорение работы

Mitogen - штука на Python, которая ускоряет Ansible в разы.
На целевом сервере запускается подобие агента, таски загружаются в один поток.
Устанавливается либо через pip либо через пакет.
Для использования надо включить его в ansible.cfg. 
Нужно включить его в директиву [defaults]:

strategy_plugins = /.../ansible_mitogen/plugins/strategy

strategy = mitogen_linear

Стратегии - linear по умолчанию (все хосты выполняют одну задачу в n потоков=n хостов одновременно), serial - пакетное выполнение на пачке хостов, free - выполнять так быстро как только можно.

## Сущности

Task
  - вызов модуля Ansible. Более 500 модулей, но в основном используется штук 10

Variables
  - позволяют писать универсальные роли

Template
  - на языке Jinja2
  - позволяют создавать файлы на разных типах серверов

Handler
  - таски в самом конце, обычно рестарт сервиса

Role
  - объединение по логическому принципу таски, переменные, темплейты и хендлеры.



# Kubernetes

## Тема №1: Знакомство с Kubernetes, основные компоненты

Новая версия K8S - 1.18. Юбилейный интенсив и первый, который выходит по этой версии.

99% инсталляций - на базе Docker.

Docker - как фура.
Docker Compose - как товарный поезд.
  - нет масштабирования
  - привязан к хосту
Kubernetes - грузовой порт.
  - логистика==оркестрация
  - учет
  - оптимизация хранения
  - отправка==scheduling

Оркестратор - основная функция K8S.


Почему K8S?
  - вырос из Google. Google==хайп и бренд.
  - удачные архитектурные решения
  - большое коммьюнити - большое количество учебных материалов, статей, мануалов, паттерны-антипаттерны, курсы итд. Чем больше инженеров, которые пишут код - тем больше векторов развития. Поиск оптимального пути и оптимальных решений развития продукта.
  - есть интеграция с экосистемой AD и Microsoft. На Windows Server 2019 работает с Flannel.


Когда нужен?
  - immutable - неизменяемость. Как контейнеры, так и ноды кластера будут работать одинаково на всех средах, где установлен Docker. K8S - несколько бинарников, не требующих внешних зависимостей и все остальные компоненты в контейнерах. 
  - декларативность. Описание в YAML, это обычные текстовые файлы -> используются обычные инструменты разработки типа GIT, версионирование, unit-тесты итд. Ложится на концепцию IaC.
  - Self-Healing - каждый компонент отвечает за свою часть инфраструктуры и поддерживает его в актуальном состоянии. Если что-то случится с контейнерами - оркестратор разберется )
  - большая распределенность и независимость компонентов кластера и приложений (Decoupling), компоненты инфраструктуры полагаются на свои SLA.

Когда не нужен?
  - K8S ради зарплаты - инженеры работающие с Docker на небольших средах - усложняют проект, часто не получая профита.
  - маленькая инфраструктура где нет горизонтального масштабирования.
  - слышали про "стандарт отрасли" - появилось желание "положить всё в Кубернетес"
  - желание получить кнопку "сделать хорошо" без налаженных процессов автоматизации, тестирования, разработки итд

Архитектура
  - основной элемент - KubeAPI сервер. Через него идет доступ в кластер плюс через него общаются компоненты кластера.
    - UI - дашборд встроенный + сторонние дашборды. В основном для разработчиков. Удобно что может генерировать ссылки на объекты. 
    - CLI - kubectl. В основном для инженеров. 
  - Kubernetes Master - это набор из нескольких компонентов, каждый из которых делает свои задачи. Бинарные файлы, которые реализуют логику работы кластера
  - Worker Node - сервера, на которых запускаются приложения.
  - ETCD - в качестве хранилища конфигурации. 
  - никакой компонент напрямую не общается друг с другом, всё через Kube API
  - никакой компонент не говорит другому, что нужно делать, всё декларативно через манифесты
  - компоненты независимы и отвязаны друг от друга

Базовые абстракции
  POD - один или группа контейнеров, объединенных в логическую единицу. Кубернетес не оперирует отдельными контейнерами. 
    - Контейнеры в поде в одном общем сетевом namespace, PID namespace. 
    - Под - минимум 2 контейнера - контейнер с приложением + технический контейнер (с процессом pause, он всегда спит и берет на себя все namespace). Если контейнер внутри пода должен быть перезапущен, то перезапущенный контейнер поднялся бы в другом неймспейсе, так как есть технический контейнер, который держит неймспейсы, то новый контейнер запускается в том же неймспейсе. Средствами K8S - его не видно, видно через Docker, CRI-O итд. 
    - POD - минимальная единица, которой может оперировать K8S
  все контейнеры пода работают на одном физическом хосте
  Когда контейнеры надо объединять в одном POD?
    - когда должны быть на одном хосте
    - когда масштабируются линейно
    - если компоненты этих контейнеров имеют сильную связь, один без другого не живет без локальной связи и ломаются если связь по сети.
    - обязательные поля в манифесте - spec:containers:name
  REPLICA SET - запускает несколько копий пода. Задача - скейлить поды. Кол-во подов считается по заданию лейблов на подах. Лейбл это key-value пара. 
    - Под в репликасете описывается в поле spec:template. Есть metadata:labels. Поле name отсутствует в template, оно будет заполняться автоматически.
    - Если выставить отдельному поду такой же лейбл, как в репликасете - то репликасет победит и под будет уничтожен.
    - Если изменить RS - то это автоматически не приведет к пересозданию подов. Работающие поды так и будут работать на старой версии. Обновить можно либо создав новый под заскейлив RS, либо удалив работающий под.
  DEPLOYMENT - самая распространенная абстракция в продакшне.
    - есть поле spec:strategy. Дефолтная стратегия - RollingUpdate (обновлять реплики по одной). Recreate - убить все реплики разом и поднять новую версию.
    - RollingUpdate:MaxSurge/MaxUnavailable - можно задавать в штуках, можно в процентах.
    - деплоймент каскадно создает RS. 
    - обновление деплоймента обновляет приложение налету, в отличие от RS!
    - обновление деплоймента создает новый RS с новыми настройками. Старый остается жить для возможности отката обновления.
    - kubectl rollout undo deployment <depl name> --to-revision=... - для отката обновлений. Работает только с деплойментами!

Пробы
  - Liveness Probe - контроль за состоянием приложения во время жизни, работает постоянно. Работает уже после того, как приложение поднялось. Если проба не прошла - приложение перезапустится.
  - Readiness Probe - принимает, готово ли приложение принимать трафик, если нет - приложение выключается из балансировки, работает постоянно. Проба прошла = под переходит в Ready.
  В жизни большинство приложений имеют одинаковые Liveness и Readiness пробы.
  - Startup Probe - появилось недавно. Для того, чтобы проверять legacy-приложение или приложения, которые долго стартуют, типа Java. Можно самостоятельно указывать, как проверять, запустилось ли приложение. 
  failureThreshold: количество последовательных неудач в пробе, чтобы она считалась неудачной. Сбросит счетчик успешных проб.
  successThreshold: количество последовательных успехов в пробе, чтобы она считалась удачной. Сбросит счетчик неуспешных проб.
  periodSeconds: с какой частотой будет происходить проверка.
  httpGet: успешными считаются 200-300е коды ответа.
  Один из подходов к limit management - запустить приложение без реквестов и лимитов и посмотреть сколько оно будет есть в максимуме. Второй подход, более правильный - это НТ.
  Некорректно выставленные лимиты и реквесты - это лучше, если не выставленные вообще. 
  У системных компонентов K8S - лимиты и реквесты либо не проставлены, либо проставлено только одно. 


Ресурсы
  - Limits - верхняя граница ресурсов, которые под может использовать на ноде
  - Requests - кол-во ресурсов, которые резервируются для пода на ноде, НЕ делятся с другими подами.
  - основные ресурсы: CPU, RAM
  - градация выделения CPU - миллиядро (millicore). Это time-based multiplexing, речь про выделение процессорного времени одного логического ядра ноды.
  - QoS Class - проставляется автоматически в зависимости от реквестов и лимитов. Есть Burstable, есть Guaranteed, есть Best Effort. Последний включается если реквестов и лимитов не задано. Если реквесты равны лимитам - то выставляется Guaranteed. Guaranteed в K8S считаются наиболее важными и критичными приложениями (так как 100% резерв ресурсов)
  Если QoS Class: Guaranteed - то у него будет очень высокий OOM Score - значит K8S будет до последнего стараться не убивать этот под при просадке кластера по памяти.

Настройка приложения.

Для stage и prod как правило нужны разные настройки приложения.

ConfigMap - YAML-файл с секцией data, в которой настройки в формате key:value
Монтирование - через VolumeMounts.
  - volumeMounts - на уровне контейнера
  - volumes - на уровне пода

Secrets - хранение в base64 некоторой чувствительной информации.
  - generic - пароли/токены для приложений
  - docker-registry - авторизация в реджистри
  - tls - сертификаты для Ingress
  Секреты нужны только для того, чтобы отделить информацию, которую нельзя хранить в ConfigMap. Особенно важно для RBAC - кому в принципе можно смотреть в секреты, кому нет. 
  Для всех секретов кроме generic - есть жестко заданные названия полей.
  Кодировка в Base64 - не только для безопасности, но и для того, чтобы спецсимволы в паролях не сломали YAML.

  Ссылаться на секреты можно в описании переменных окружения: spec:containers:env
  - name: <NAME>
    valueFrom:
      secretKeyRef:
        name: <secretName>
        key: <значение в секрете>

Service - это абстракция, которая позволяет обращаться к любому однотипному поду за ним. По сути - это логический балансировщик. 
  - сопоставление объектов как и в деплойментах - по лейблам (меткам)
  - в сервисе указывается selector:<набор лейблов>
  - у сервиса есть раздел ports - это набор портов, которые обслуживает сервис. -port - порт, на котором принимается трафик. targetPort - порт, открытый на поде, куда надо слать трафик. 
  - ClusterIP - поле, которое добавляется кластером K8S, срабатывает Service Discovery и присваивается динамический IP-адрес, который добавляется в манифест сервиса в это поле.
  - работает через IPTables и NAT

Endpoints - абстракция, которая создается автоматически при создании сервиса. Это список IP-адресов, на которые надо балансировать запросы при обращении к сервису. 
Если ReadinessProbe не проходит, то под удаляется из перечня эндпойнтов. Но не мгновенно. 

Ingress - работает на L7. Работает только при наличии в кластере Ingress Controller-а. 
  - самый популярный - на базе nginx (community)
  - есть еще авторский от самой компании Nginx
  - есть Envoy, Traefik, HAProxy и облачные разные варианты.
  - сущность, которая описывает правила маршрутизации HTTP-запросов. 
  - описываем "что пришло - куда послать"
  - ингресс принимает запрос и отправляет его на сервис

С версии 1.18 класс IngressController-ов можно указывать в манифесте.

PV/PVC
  - PV описывает информацию по конкретному тому
  - PVC - требования на подключения к тому
  - указывается StorageCLass, если не указывается то подключается та СХД что по умолчанию
  - Claim это прокси между PV и подом.
  - Если закончились PV в кластере, то под не запустится
  - provisioner - специальное ПО, которое умеет ходить в СХД, создавать диски и делать из них PV. Использование provisioner-а позволяет откусывать ровно столько места от СХД, сколько нужно поду.
  - ReadWriteOnce - эксклюзивный доступ на запись. Подмонтирован может быть только к одному узлу.
  - ReadWriteMany - множественный доступ на запись, блокировки разруливаются на уровне самой СХД

initContainers - специальные контейнеры, которые запускаются перед приложением. Например - чтобы поправить права на доступ к томам, чтобы смигрировать БД, доп. настройки (куда-то сходить, что-то дёрнуть и так далее)

Компоненты кластера
  - ETCD
    - key-value хранилище обо всей информации о кластере
    - порт 2379 для подключения клиентов и 2380 для взаимодействия между нодами
    - 2 версии API - v2/v3. 
    - строго требует быстрых дисков.
  - API Server
    - центральный компонент K8S
    - единственный, кто может общаться и писать в ETCD напрямую
    - обычный REST API, поддерживает HTTP запросы
    - аутентификация и авторизация
  - Controller Manager
    - набор контроллеров
    - Node Controller - управление нодами
    - Replicaset Controller
    - Endpoints Controller - создает эндпойнты для сервисов
    - Garbage Collector - сборщик мусора
    - другие...
    - всегда смотрит в API Server и мониторит создания новых объектов. (watch - подписка на события)
  - Scheduler
    - назначает запуск POD на ноды, учитывая:
      - QoS
      - affinity/anti-affinity
      - Priority Class (DEV/Prod)
      - выделяет ресурсы
    - отслеживает (watch) создание новых подов в API Server. Назначает NodeName для новых подов и обновляет манифест.
  - Kubelet
    - работает на всех серверах кластера
    - node agent
    - работает не внутри контейнера
    - systemd приложение на хосте
    - отдает команду docker daemon-у
    - фактически создает поды
  - Kube-Proxy
    - смотрит в Kube-API
    - стоит на всех хостах
    - управляет сетевыми правилами на нодах
    - реализует правила (IPtables - олдскул, IPVS - стильно модно молодежно)
  - контейнеризация
  - сеть
  - DNS
   - для каждого сервиса создается имя myservice.mynamespace.svc.cluster.local
   - DNS в кластере работает так же через Service

Service это не прокси!
В Service определяются правила iptables для роутинга
Проблемы NAT в Linux - может быть такое, что отправленные разные пакеты записались с одинаковым исходящим портом в таблицу, обратный пакет пришедший позже - будет отброшен. 
Поэтому появился IPVS. 

Network
  - задача сетевого плагина - связать все поды и все ноды друг с другом
  - все сетевые плагины раздают IP адреса подам
  - Flannel в режиме Host Gateway раздает маршруты
  - работает только там где сервера связаны друг с другом по L2
  - Network Policy - этим подам ходить туда, а этим подам туда нельзя (умеет Calico)
  - если IP адреса у вас кончились и у вас Flannel - то все плохо и надо расширять хостовую сеть. Если Calico - то есть возможность добавить дополнительные диапазоны. 
  - лучше побольше нод, поменьше подов на ноду
  - плагины реализуют шифрование между нодами

Ingress
  - это абстракция, как внешний трафик будет попадать и распределяться на поды приложения
  - это манифест, в котором описывается куда трафик приходит и куда попадает
  - в стандартном ингрессе на базе nginx внутри конфига будут proxy_pass-ы на апстримы в виде сервисов
  - по факту там не nginx а openresty, так как мягкий релоад конфига только в nginx plus
  - есть второй фоновый процесс, который занимается релоадом конфига


Отказоустойчивый сетап Control Plane
  - ETCD - 3 штуки минимум, быстрые диски, быстрая сеть
  - API Server - по кол-ву ETCD, чтобы отправлял данные быстро в локальную копию ETCD
  - Controller Manager - только один инстанс работает в момент времени. 
  - Scheduler - только один инстанс работает в момент времени. 
  - Kubelet и Kube-proxy - работают с одним API-сервером. Перед ними на каждой ноде запускают контейнер с Nginx, на котором делают proxy_pass на upstream группы API Server-ов. 


Развертывание кластера через Kubespray
  - Rancher хорошо, но часто ломается
  - kubeadm хорошо, от авторов K8S, но требует много ручного труда
  - kubespray ставит отказоустойчивый кластер одной кнопкой
  - kubespray раньше ставил и управлял пакетами, сейчас половина кубеспрея это вызов kubeadm с разными ключами
  - нет возможности апгрейда кластера, развернутого по классике- серты выписываются на 1 год, обновлять вручную (сертов выпускается много). Сейчас функционал допилен, серты обновляются одной командой. Также серты обновляются при процедуре обновления кластера.
  - если установка вылетела по ошибке - повторный запуск может не помочь
  - подготовка серверов:
    - kernel 4.x
    - disable firewall
    - local net
    - disable swap
    - master - 2CPU, 4GB RAM
    - Ingress - 1CPU, 2GB RAM
    - Node - 4CPU, 8GB RAM
  - etcd можно ставить не только на мастерах, но и на выделенных нодах
  - опция download_run_once: true //скачивание один раз и потом распространение локально по узлам кластера
  - опция etcd_memory_limit: 0 //раньше при превышении потребления памяти в 512МБ контейнеры падали. Лучше пусть он съест всю память на хосте, чем упадет.
  - используется все-таки IPTables
  - 



### Полезные команды

kubectl create - создает новые объекты
kubectl apply - работает так же как create если объекта нет, если есть - обновляет.

kubectl describe - универсальная команда диагностики. Если что-то не так - смотрим Events.
kubectl get po --show-labels //показать лейблы подов

export EDITOR=vim //задает редактор по умолчанию для kubectl eidt




## Тема №2: Устройство кластера, основные компоненты, отказоустойчивость, сеть k8s



## Тема №3: Kubespray, тюнинг и настройка кластера Kubernetes



## Тема №4: Продвинутые абстракции Kubernetes



## Тема №5: DNS в кластере. Публикация сервисов и приложений



## Тема №6: Работа с Helm



## Тема №7: Ceph, установка в режиме делай как я. Работа с постоянными томами, sc, pvc, pv, подключение томов к подам. Разбор на примере Ceph



## Тема №8: Установка cert-manager



## Тема №9: Обслуживание кластера



## Тема №10: Практическая работа, докеризация приложения и запуск в кластере

