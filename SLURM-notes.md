# SLURM - заметки

# Docker

Хорошая практика - запускать процессы внутри контейнера в foreground (при этом сам запущенный контейнер может работать и в background)
Слои в образе - readonly
.dockerignore - указывать директории, которые не надо класть в образ
Отключение кэшей - для пакетных менеджеров и для сборщиков типа pip
Например для APT внутри директивы установки пакетов - добавить

`RUN apt-get update && apt-get install ... && rm -rf /var/lib/apt/lists/*`

Концепция контейнеров не предполагает заходить внутрь контейнера для сбора и прочтения логов - логи пишутся в stdout/stderr
Все что контейнер написал в stdout/stderr - docker переведет в JSON, добавит метаинформацию и сложит на хосте в определенную директорию
Для того чтобы ловить логи nginx из стандартных файлов куда он их складывает - можно добавить директиву

`RUN set -ex && \
	ln -sf /dev/stdout /var/log/nginx/access.log && \
	ln -sf /dev/stderr /var/log/nginx/error.log`

set -ex - это чтобы видеть, какие команды в какой момент выполняются
Разница между COPY и ADD - ADD может скопировать файлы из удаленного источника, разархивировать архив и так далее. Если надо просто скопировать файл с хоста в контейнер - проще использовать COPY.
Разница между ENTRYPOINT и CMD - последняя чаще всего используется так же как ENTRYPOINT, но есть нюансы
ENTRYPOINT - должен использоваться при старте контейнера
CMD - добавлять параметры для энтрипойнта
То есть запуск приложения - через ENTRYPOINT, а ключи для запуска передать в CMD

Docker не про контейнеры. Docker про стандартизацию поставки и распространению ПО.
Воспроизводимость, консистентность.

Docker engine - это демон
Запущен как обычный процесс
docker build, ps, run - консольные утилиты, передают GRPC-команды на докер-демон
На том же хосте утилита обращается в UNIX-сокет локальной машины

Docker registry - хранилище images

## Namespace Isolation

Namespace - механизм изоляции процессов друг от друга, вшит в ядро Linux - PID, Net, Mount, User
  - PID NS - пространство имен айди процессов. PID 1 - это корневой процесс, без него работа системы завершается. Если скрипт внутри контейнера конечный - контейнер завершается (по умолчанию запуск под PID 1).
  - NET NS - сетевое окружение: сетевые интерфейсы (+lo), route tables, iptables, sockets
  - в один NS можно добавлять несколько процессов (например несколько контейнеров объединить общим localhost через который контейнеры могут общаться)
  - MNT NS - своя root fs, свои приватные маунты
  - правила сетевого NS можно тюнить в части параметров sysctl для каждого контейнера отдельно
  - USER NS - позволяет мапить GID/UID с хоста

Докер это в первую очередь система упаковки и доставки, а не обеспечения безопасности и защиты контейнеров.
Докер не обеспечивает суперизоляцию процессов и их безопасность, это скорее про ВМ или физику.

CGroups - механизм изоляции вычислительных ресурсов процессов друг от друга - CPU, Memory, I/O, Net
  - добавляют overhead - или нет. В зависимости от ситуации

Copy-On-Write - тоже механика ядра Linux. При чтении используется общая копия, при записи - новая копия. Используется также в LXC

## Storage-драйверы

  - самые популярные - overlay, devicemapper, AUFS
  - devicemapper - в CentOS дефолтный до недавнего времени, работает через loopback, надо использовать в версии с SYN-пулами. 
  - overlay - хорошая поддержка лишь в ядре 4.X. Поддерживается в ядре от 3.10 - CentOS 7.4. Для 7.4 рекомендуется overlay2.
  - для Ubuntu и Debian - лучший драйвер это AUFS.
  - BTRFS/ZFS - первый не развивается, второй под Linux практически тоже. 

## Log-драйверы.

  - по умолчанию - JSON-файл, со своей задачей справляется.
  - journald - из системного журнала. Нужно тюнить, лимиты по умолчанию на прием логов скромные. Плюс к тому есть хард-лимиты - выше определенных значений его не затюнить и поэтому лучше его не использовать.
  - syslog - по UDP/TCP/TLS
  - fluentd - по TCP, можно собирать логи из всех источников одновременно
  - ETW -> Windows Event Store
  - Splunk
  - Geif
  - Logentries
  - Amazon CloudWatch
  - Google Cloud
  - из драйвера логи попадают в log shipper.
  - рекомендуется для Docker остаться на JSON-файле (не теряя возможности выполнять docker logs), собирать логи через fluentd, потом в ElasticSearch + Kibana для просмотра.

Полезные опции
  - dockerd --default-ulimit - аналог docker run -ulimit, переопределяет максимальное число процессов, которые может запускать контейнер.
  - --dns - переопределять DNS-серверы
  - --insecure-registry - если свой реестр без сертификата, по умолчанию docker не даст подключиться к реестру без TLS.

## Docker Compose

Это обертка на Python над Docker - позволяет по описанию в YAML запускать многоконтейнерную структуру.

В 3 версии файла (версия декларируется параметром version: в YAML) выпилили поддержку healthcheck-ов и зависимостей.

healthcheck:
  test: ["CMD", "команда", "параметры"]
  interval: 1s
  timeout: 1s
  retries: 60

Healthcheck - это команда, которая должна возвращать нулевой exit-код. Если не возвращает - повторяется с нужным интервалом, после нужного таймаута и нужное количество раз.

docker-compose.override.yml - применяется автоматически при docker-compose up без указания дополнительных файлов.
Как пример - можно описать пробрасываемые порты и подключаемые тома с локального хоста при локальной разработке.
docker-compose.test.yml - нужно руками указывать docker-compose up -f <файл>. Оверрайд при этом отключается.
Как пример - использование переменных для тестового окружения.

## Docker registry

Альтернативные продукты - Harbor
  - хранение образов
  - авторизация
  - ротация
  - сканирование 
  - hook API
  - хранение других сущностей

Можно указать альтернативный registry в docker run, по умолчанию образы берутся с hub.docker.io

## Лучшие практики, паттерны и антипаттерны

Файлы, где описаны зависимости - копируются в начале
Логично что после копирования файлов - надо делать установку зависимостей, а потом копировать сам код.
Если копировать скрипты например из Debian на Alpine - то надо следить как именуются пользователи. В Debian nginx запускается под пользователем www-data, в Alpine - под nginx.
Добавляя пользователя внутрь контейнера - усложняется процесс сборки, если можно без этого обойтись - то надо обходиться.

Update делать нормально, upgrade - нет, так как это плохое, непредсказуемое поведение. При каждой сборке образа будет апгрейд пакетов, и что-то может поломаться.

Dev-пакеты для того чтобы собрать зависимости - лучше не делать в образе, в котором будет запускаться приложение. Лучше их собирать в новый базовый образ.

Надо отключать кэши и не тащить в образ.

Пакеты надо ставить до добавления кода, если так не делать - любое изменение кода будет вызывать повторную выкачку и установку пакетов.

Бандлы пакетов лучше выносить в переменные и указывать в начале Dockerfile, чтобы можно было потом легко вносить изменения:

`ARG BUILD_PACKAGES="nginx nodejs dcron tzdata postgresql-dev libxslt-dev"`

Статику можно собирать либо в Dockerfile, либо в ENTRYPOINT.
Лучше в Dockerfile, так как контейнеры могут падать, рестартоваться, а смысл контейнеров - как можно быстрее поднять работающее приложение.

В идеале 1 контейнер = 1 процесс. В реальной жизни часто приходится искать компромиссы. 

Приложения надо адаптировать. Если приложение не писалось под Docker - есть ненулевая вероятность, что оно не будет нормально работать. Пример - Java 7, которая не могла правильно определять количество процессоров внутри контейнера.

Архитектуру надо прорабатывать - как с точки зрения инфраструктуры, так и с точки зрения приложений и взаимодействия между ними. Как пример - делить приложение на фронт и бэк с общением между ними по HTTP.

Docker надо мониторить, причем не так как сервера. Контейнеры - штука непостоянная. Нужен autodiscovery, чтобы следить за новыми контейнерами. Нужно мониторить сами сервисы - следить за доступностью сервиса, а не конкретного контейнера. 

Чтобы успешно управлять контейнерной инфраструктурой - надо много дополнительных сущностей:
  - Nomad
  - Docker Swarm
  - Kubernetes
  - мониторинг
  - логирование


# Ansible

Плюсы:
  - не нужен агент
  - работа по SSH
  - не нужно программировать - вся настройка в YAML
  - идемпонентность - описывается состояние в которое должна быть приведена система и в идеале данное состояние должно быть идентичным для разных запусков в разное время для системы.
  - нужен только Python

Минусы:
  - нет агента - задачи загружаются по одной и в том порядке что указал создатель плейбука
  - нужен Python
  - YAML - нельзя делать сложную логику и надо следить за пробелами и отступами

Shell-скриптами можно заменить какие-то простые задачи, но в конечном итоге можно увлечься и написать свой Ansible, в котором будут проблемы:
  - время на изобретение велосипеда
  - затраты сотрудников на изучение этого велосипеда

Установить можно через pip (pip install ansible)

`ansible -m setup <host>` - сбор фактов с сервера

`ansible -m ping -i red all -u bond -k`
  - -m - модуль
  - -i - инвентори
  - all - все хосты во всех группах в инвентори
  - -u - пользователь
  - -k - запросить пароль

## Сущности Ansible

Playbook
ключи:
  - --diff - показать какие изменения сделал таск (что было и что стало)
  - --force-handlers - выполнять хендлеры независимо от того, завершился сценарий успешно или неуспешно. По умолчанию сценарий останавливается на ошибке.
  - --inventory - указатель на инвентарь
  - --limit - выполнять задачи не на всех серверах а на определенных группах или определенных хостах
  - --step - используется редко, пошаговое выполнение тасков
  - --become - сделать sudo от имени пользователя, повышение привилегий, можно написать и в инвентаре, и в таске, и в плейбуке
  - --ask-pass - короткая форма -k, запрос на пароль при подключении по ssh

Inventory

Group

## Ускорение работы

Mitogen - штука на Python, которая ускоряет Ansible в разы.
На целевом сервере запускается подобие агента, таски загружаются в один поток.
Устанавливается либо через pip либо через пакет.
Для использования надо включить его в ansible.cfg. 
Нужно включить его в директиву [defaults]:

strategy_plugins = /.../ansible_mitogen/plugins/strategy

strategy = mitogen_linear

Стратегии - linear по умолчанию (все хосты выполняют одну задачу в n потоков=n хостов одновременно), serial - пакетное выполнение на пачке хостов, free - выполнять так быстро как только можно.

## Сущности

Task
  - вызов модуля Ansible. Более 500 модулей, но в основном используется штук 10

Variables
  - позволяют писать универсальные роли

Template
  - на языке Jinja2
  - позволяют создавать файлы на разных типах серверов

Handler
  - таски в самом конце, обычно рестарт сервиса

Role
  - объединение по логическому принципу таски, переменные, темплейты и хендлеры.



# Kubernetes

## Тема №1: Знакомство с Kubernetes, основные компоненты

Новая версия K8S - 1.18. Юбилейный интенсив и первый, который выходит по этой версии.

99% инсталляций - на базе Docker.

Docker - как фура.
Docker Compose - как товарный поезд.
  - нет масштабирования
  - привязан к хосту
Kubernetes - грузовой порт.
  - логистика==оркестрация
  - учет
  - оптимизация хранения
  - отправка==scheduling

Оркестратор - основная функция K8S.


Почему K8S?
  - вырос из Google. Google==хайп и бренд.
  - удачные архитектурные решения
  - большое коммьюнити - большое количество учебных материалов, статей, мануалов, паттерны-антипаттерны, курсы итд. Чем больше инженеров, которые пишут код - тем больше векторов развития. Поиск оптимального пути и оптимальных решений развития продукта.
  - есть интеграция с экосистемой AD и Microsoft. На Windows Server 2019 работает с Flannel.


Когда нужен?
  - immutable - неизменяемость. Как контейнеры, так и ноды кластера будут работать одинаково на всех средах, где установлен Docker. K8S - несколько бинарников, не требующих внешних зависимостей и все остальные компоненты в контейнерах. 
  - декларативность. Описание в YAML, это обычные текстовые файлы -> используются обычные инструменты разработки типа GIT, версионирование, unit-тесты итд. Ложится на концепцию IaC.
  - Self-Healing - каждый компонент отвечает за свою часть инфраструктуры и поддерживает его в актуальном состоянии. Если что-то случится с контейнерами - оркестратор разберется )
  - большая распределенность и независимость компонентов кластера и приложений (Decoupling), компоненты инфраструктуры полагаются на свои SLA.

Когда не нужен?
  - K8S ради зарплаты - инженеры работающие с Docker на небольших средах - усложняют проект, часто не получая профита.
  - маленькая инфраструктура где нет горизонтального масштабирования.
  - слышали про "стандарт отрасли" - появилось желание "положить всё в Кубернетес"
  - желание получить кнопку "сделать хорошо" без налаженных процессов автоматизации, тестирования, разработки итд

Архитектура
  - основной элемент - KubeAPI сервер. Через него идет доступ в кластер плюс через него общаются компоненты кластера.
    - UI - дашборд встроенный + сторонние дашборды. В основном для разработчиков. Удобно что может генерировать ссылки на объекты. 
    - CLI - kubectl. В основном для инженеров. 
  - Kubernetes Master - это набор из нескольких компонентов, каждый из которых делает свои задачи. Бинарные файлы, которые реализуют логику работы кластера
  - Worker Node - сервера, на которых запускаются приложения.
  - ETCD - в качестве хранилища конфигурации. 
  - никакой компонент напрямую не общается друг с другом, всё через Kube API
  - никакой компонент не говорит другому, что нужно делать, всё декларативно через манифесты
  - компоненты независимы и отвязаны друг от друга

Базовые абстракции
  POD - один или группа контейнеров, объединенных в логическую единицу. Кубернетес не оперирует отдельными контейнерами. 
    - Контейнеры в поде в одном общем сетевом namespace, PID namespace. 
    - Под - минимум 2 контейнера - контейнер с приложением + технический контейнер (с процессом pause, он всегда спит и берет на себя все namespace). Если контейнер внутри пода должен быть перезапущен, то перезапущенный контейнер поднялся бы в другом неймспейсе, так как есть технический контейнер, который держит неймспейсы, то новый контейнер запускается в том же неймспейсе. Средствами K8S - его не видно, видно через Docker, CRI-O итд. 
    - POD - минимальная единица, которой может оперировать K8S
  все контейнеры пода работают на одном физическом хосте
  Когда контейнеры надо объединять в одном POD?
    - когда должны быть на одном хосте
    - когда масштабируются линейно
    - если компоненты этих контейнеров имеют сильную связь, один без другого не живет без локальной связи и ломаются если связь по сети.
    - обязательные поля в манифесте - spec:containers:name
  REPLICA SET - запускает несколько копий пода. Задача - скейлить поды. Кол-во подов считается по заданию лейблов на подах. Лейбл это key-value пара. 
    - Под в репликасете описывается в поле spec:template. Есть metadata:labels. Поле name отсутствует в template, оно будет заполняться автоматически.
    - Если выставить отдельному поду такой же лейбл, как в репликасете - то репликасет победит и под будет уничтожен.
    - Если изменить RS - то это автоматически не приведет к пересозданию подов. Работающие поды так и будут работать на старой версии. Обновить можно либо создав новый под заскейлив RS, либо удалив работающий под.
  DEPLOYMENT - самая распространенная абстракция в продакшне.
    - есть поле spec:strategy. Дефолтная стратегия - RollingUpdate (обновлять реплики по одной). Recreate - убить все реплики разом и поднять новую версию.
    - RollingUpdate:MaxSurge/MaxUnavailable - можно задавать в штуках, можно в процентах.
    - деплоймент каскадно создает RS. 
    - обновление деплоймента обновляет приложение налету, в отличие от RS!
    - обновление деплоймента создает новый RS с новыми настройками. Старый остается жить для возможности отката обновления.
    - kubectl rollout undo deployment <depl name> --to-revision=... - для отката обновлений. Работает только с деплойментами!

Пробы
  - Liveness Probe - контроль за состоянием приложения во время жизни, работает постоянно. Работает уже после того, как приложение поднялось. Если проба не прошла - приложение перезапустится.
  - Readiness Probe - принимает, готово ли приложение принимать трафик, если нет - приложение выключается из балансировки, работает постоянно. Проба прошла = под переходит в Ready.
  В жизни большинство приложений имеют одинаковые Liveness и Readiness пробы.
  - Startup Probe - появилось недавно. Для того, чтобы проверять legacy-приложение или приложения, которые долго стартуют, типа Java. Можно самостоятельно указывать, как проверять, запустилось ли приложение. 
  failureThreshold: количество последовательных неудач в пробе, чтобы она считалась неудачной. Сбросит счетчик успешных проб.
  successThreshold: количество последовательных успехов в пробе, чтобы она считалась удачной. Сбросит счетчик неуспешных проб.
  periodSeconds: с какой частотой будет происходить проверка.
  httpGet: успешными считаются 200-300е коды ответа.
  Один из подходов к limit management - запустить приложение без реквестов и лимитов и посмотреть сколько оно будет есть в максимуме. Второй подход, более правильный - это НТ.
  Некорректно выставленные лимиты и реквесты - это лучше, если не выставленные вообще. 
  У системных компонентов K8S - лимиты и реквесты либо не проставлены, либо проставлено только одно. 


Ресурсы
  - Limits - верхняя граница ресурсов, которые под может использовать на ноде
  - Requests - кол-во ресурсов, которые резервируются для пода на ноде, НЕ делятся с другими подами.
  - основные ресурсы: CPU, RAM
  - градация выделения CPU - миллиядро (millicore). Это time-based multiplexing, речь про выделение процессорного времени одного логического ядра ноды.
  - QoS Class - проставляется автоматически в зависимости от реквестов и лимитов. Есть Burstable, есть Guaranteed, есть Best Effort. Последний включается если реквестов и лимитов не задано. Если реквесты равны лимитам - то выставляется Guaranteed. Guaranteed в K8S считаются наиболее важными и критичными приложениями (так как 100% резерв ресурсов)
  Если QoS Class: Guaranteed - то у него будет очень высокий OOM Score - значит K8S будет до последнего стараться не убивать этот под при просадке кластера по памяти.

Настройка приложения.

Для stage и prod как правило нужны разные настройки приложения.

ConfigMap - YAML-файл с секцией data, в которой настройки в формате key:value
Монтирование - через VolumeMounts.
  - volumeMounts - на уровне контейнера
  - volumes - на уровне пода

Secrets - хранение в base64 некоторой чувствительной информации.
  - generic - пароли/токены для приложений
  - docker-registry - авторизация в реджистри
  - tls - сертификаты для Ingress
  Секреты нужны только для того, чтобы отделить информацию, которую нельзя хранить в ConfigMap. Особенно важно для RBAC - кому в принципе можно смотреть в секреты, кому нет. 
  Для всех секретов кроме generic - есть жестко заданные названия полей.
  Кодировка в Base64 - не только для безопасности, но и для того, чтобы спецсимволы в паролях не сломали YAML.

  Ссылаться на секреты можно в описании переменных окружения: spec:containers:env
  - name: <NAME>
    valueFrom:
      secretKeyRef:
        name: <secretName>
        key: <значение в секрете>

Service - это абстракция, которая позволяет обращаться к любому однотипному поду за ним. По сути - это логический балансировщик. 
  - сопоставление объектов как и в деплойментах - по лейблам (меткам)
  - в сервисе указывается selector:<набор лейблов>
  - у сервиса есть раздел ports - это набор портов, которые обслуживает сервис. -port - порт, на котором принимается трафик. targetPort - порт, открытый на поде, куда надо слать трафик. 
  - ClusterIP - поле, которое добавляется кластером K8S, срабатывает Service Discovery и присваивается динамический IP-адрес, который добавляется в манифест сервиса в это поле.
  - работает через IPTables и NAT

Endpoints - абстракция, которая создается автоматически при создании сервиса. Это список IP-адресов, на которые надо балансировать запросы при обращении к сервису. 
Если ReadinessProbe не проходит, то под удаляется из перечня эндпойнтов. Но не мгновенно. 

Ingress - работает на L7. Работает только при наличии в кластере Ingress Controller-а. 
  - самый популярный - на базе nginx (community)
  - есть еще авторский от самой компании Nginx
  - есть Envoy, Traefik, HAProxy и облачные разные варианты.
  - сущность, которая описывает правила маршрутизации HTTP-запросов. 
  - описываем "что пришло - куда послать"
  - ингресс принимает запрос и отправляет его на сервис

С версии 1.18 класс IngressController-ов можно указывать в манифесте.

PV/PVC
  - PV описывает информацию по конкретному тому
  - PVC - требования на подключения к тому
  - указывается StorageCLass, если не указывается то подключается та СХД что по умолчанию
  - Claim это прокси между PV и подом.
  - Если закончились PV в кластере, то под не запустится
  - provisioner - специальное ПО, которое умеет ходить в СХД, создавать диски и делать из них PV. Использование provisioner-а позволяет откусывать ровно столько места от СХД, сколько нужно поду.
  - ReadWriteOnce - эксклюзивный доступ на запись. Подмонтирован может быть только к одному узлу.
  - ReadWriteMany - множественный доступ на запись, блокировки разруливаются на уровне самой СХД

initContainers - специальные контейнеры, которые запускаются перед приложением. Например - чтобы поправить права на доступ к томам, чтобы смигрировать БД, доп. настройки (куда-то сходить, что-то дёрнуть и так далее)


### Полезные команды

kubectl create - создает новые объекты
kubectl apply - работает так же как create если объекта нет, если есть - обновляет.

kubectl describe - универсальная команда диагностики. Если что-то не так - смотрим Events.
kubectl get po --show-labels //показать лейблы подов

export EDITOR=vim //задает редактор по умолчанию для kubectl eidt



## Тема №2: Устройство кластера, основные компоненты, отказоустойчивость, сеть k8s

CoreDNS - сервис, отвечающий за Service Discovery

Компоненты кластера
  - ETCD
    - key-value хранилище обо всей информации о кластере
    - порт 2379 для подключения клиентов и 2380 для взаимодействия между нодами
    - 2 версии API - v2/v3. 
    - строго требует быстрых дисков.
  - API Server
    - центральный компонент K8S
    - единственный, кто может общаться и писать в ETCD напрямую
    - обычный REST API, поддерживает HTTP запросы
    - аутентификация и авторизация
  - Controller Manager
    - набор контроллеров
    - Node Controller - управление нодами
    - Replicaset Controller
    - Endpoints Controller - создает эндпойнты для сервисов
    - Garbage Collector - сборщик мусора
    - другие...
    - всегда смотрит в API Server и мониторит создания новых объектов. (watch - подписка на события)
  - Scheduler
    - назначает запуск POD на ноды, учитывая:
      - QoS
      - affinity/anti-affinity
      - Priority Class (DEV/Prod)
      - выделяет ресурсы
    - отслеживает (watch) создание новых подов в API Server. Назначает NodeName для новых подов и обновляет манифест.
  - Kubelet
    - работает на всех серверах кластера
    - node agent
    - работает не внутри контейнера
    - systemd приложение на хосте
    - отдает команду docker daemon-у
    - фактически создает поды
  - Kube-Proxy
    - смотрит в Kube-API
    - стоит на всех хостах
    - управляет сетевыми правилами на нодах
    - реализует правила (IPtables - олдскул, IPVS - стильно модно молодежно)
  - контейнеризация
  - сеть
  - DNS
   - для каждого сервиса создается имя myservice.mynamespace.svc.cluster.local
   - DNS в кластере работает так же через Service

Service это не прокси!
В Service определяются правила iptables для роутинга
Проблемы NAT в Linux - может быть такое, что отправленные разные пакеты записались с одинаковым исходящим портом в таблицу, обратный пакет пришедший позже - будет отброшен. 
Поэтому появился IPVS. 

Network
  - задача сетевого плагина - связать все поды и все ноды друг с другом
  - все сетевые плагины раздают IP адреса подам
  - Flannel в режиме Host Gateway раздает маршруты
  - работает только там где сервера связаны друг с другом по L2
  - Network Policy - этим подам ходить туда, а этим подам туда нельзя (умеет Calico)
  - если IP адреса у вас кончились и у вас Flannel - то все плохо и надо расширять хостовую сеть. Если Calico - то есть возможность добавить дополнительные диапазоны. 
  - лучше побольше нод, поменьше подов на ноду
  - плагины реализуют шифрование между нодами

Ingress
  - это абстракция, как внешний трафик будет попадать и распределяться на поды приложения
  - это манифест, в котором описывается куда трафик приходит и куда попадает
  - в стандартном ингрессе на базе nginx внутри конфига будут proxy_pass-ы на апстримы в виде сервисов
  - по факту там не nginx а openresty, так как мягкий релоад конфига только в nginx plus
  - есть второй фоновый процесс, который занимается релоадом конфига


Отказоустойчивый сетап Control Plane
  - ETCD - 3 штуки минимум, быстрые диски, быстрая сеть
  - API Server - по кол-ву ETCD, чтобы отправлял данные быстро в локальную копию ETCD
  - Controller Manager - только один инстанс работает в момент времени. 
  - Scheduler - только один инстанс работает в момент времени. 
  - Kubelet и Kube-proxy - работают с одним API-сервером. Перед ними на каждой ноде запускают контейнер с Nginx, на котором делают proxy_pass на upstream группы API Server-ов. 


## Тема №3: Kubespray, тюнинг и настройка кластера Kubernetes

Развертывание кластера через Kubespray
  - Rancher хорошо, но часто ломается
  - kubeadm хорошо, от авторов K8S, но требует много ручного труда
  - kubespray ставит отказоустойчивый кластер одной кнопкой
  - kubespray раньше ставил и управлял пакетами, сейчас половина кубеспрея это вызов kubeadm с разными ключами
  - нет возможности апгрейда кластера, развернутого по классике- серты выписываются на 1 год, обновлять вручную (сертов выпускается много). Сейчас функционал допилен, серты обновляются одной командой. Также серты обновляются при процедуре обновления кластера.
  - если установка вылетела по ошибке - повторный запуск может не помочь
  - подготовка серверов:
    - kernel 4.x
    - disable firewall
    - local net
    - disable swap
    - master - 2CPU, 4GB RAM
    - Ingress - 1CPU, 2GB RAM
    - Node - 4CPU, 8GB RAM
  - etcd можно ставить не только на мастерах, но и на выделенных нодах
  - опция download_run_once: true //скачивание один раз и потом распространение локально по узлам кластера
  - опция etcd_memory_limit: 0 //раньше при превышении потребления памяти в 512МБ контейнеры падали. Лучше пусть он съест всю память на хосте, чем упадет.
  - используется все-таки IPTables
  - 

## Тема №4: Продвинутые абстракции Kubernetes


### DaemonSet

Задача на примере мониторинга:
  - на каждой ноде автоматически запускается агент
  - управляются агенты из одной точки
  - конфигурируются из одной точки

Варианты решения:
  - Static Pod - был придуман для решения проблемы курицы и яйца. Когда рождается первый kubelet, он обращается к API Server, чтобы узнать, что надо запускать. Но API Server-а еще нет в природе. Для этого придумали костыль Static Pod, kubelet идет в специальный каталог с манифестами подов, и запускает их у себя на узле. Это манифесты для запуска системных компонентов кластера - ETCD, kube-proxy, nginx-proxy, API Server, итд. 
  Есть проблемы с управлением и конфигурированием - нельзя удалить, нельзя поменять настройки из одного места - надо идти на узел и менять настройки в файлах на каждом узле.   
  - Pod Anti-Affinity - как рассказать планировщику (scheduler) чтобы не запускать несколько копий на одном и том же узле. Проблемы с ситуацией, когда добавляются узлы в кластер. Надо сходить в деплоймент и вручную увеличить количество реплик по числу узлов.
  - Daemon Set - нет поля Replicas - под в составе демонсета работает на каждом узле кластера. 

Используется для всех системных задач, где нужно обеспечить работу на каждом узле кластера:
  - кэширующий DNS
  - сетевые плагины
  - аудит
  - storage plugin
  - ...

Как работает:
  - запускает поды на всех нодах кластера
  - описание соответствует Deployment
  - при добавлении ноды добавляет под
  - при удалении ноды GC удаляет под


Taint и Tolerations - механизм регулирования, на каких нодах можно запускать конкретные деплойменты, на каких нельзя.

Как игнорировать Taints на узлах
  - effect: NoSchedule
    operator: Exists

NoSchedule - только на запуск новых подов.
Toleration - это снятие запрета для конкретного Taint на ноде.

'-' (минус) в конце команды - снятие/удаление объекта. Пример для Taint:
`kubectl taint nodes foo dedicated:NoSchedule-`



### StatefulSet

Позволяет запускать группу подов (как Deployment)
Поды, которые хранят свое состояние
  - гарантирует их уникальность
  - гарантирует их последовательность

PVC Template
  - при удалении не удаляет PVC

Используется для stateful-приложений:
  - Rabbit
  - DB
  - Redis
  - Kafka
  - ...

При создании SS для каждого пода генерируется собственный PVC.
Название пода - имя SS и индекс (с нуля)

LocalVolume - это альтернатива небезопасному HostPath.
В HostPath можно было к примеру, утащить корень узла, со всеми сертификатами.


Affinity
  - nodeAffinity - позволяет задавать условия запуска пода на узлах
  - requiredDuringSchedulingIgnoredDuringExecution - для жесткого шедулинга пода на узлах
  - preferredDuringSchedulingIgnoredDuringExecution - для мягкого шедулинга (постараюсь запустить на нужной ноде, ну а там как пойдёт)
  - topologyKey - ключ, который должен быть разный на всех подах. Обычно - имя узла (kubernetes.io/hostname)

Удаление StatefulSet - по одному, в обратном порядке от создания.
PVC делать на NFS - очень рискованно.
  - проблемы с диспетчеризацией и блокировками
  - работает нормально только на pNFS + Trident через StorageClass (NetApp)
  - может довести клиентов до kernel panic
  - поднимает в потолок iowait

### Job/CronJob

Job - для тех задач, которые надо выполнить один раз без периодичности. Например - миграция БД.
  - создает под для выполнения задачи
  - перезапускает поды до успешного выполнения задачи
  - или до истечения таймаутов: activeDeadLineSeconds (лимит на время работы всего Job, неважно сколько подов и сколько раз под запускался и падал) или backoffLimit (сколько раз Job может падать)

CronJob - для задач, которые надо выполнять по расписанию 
  - startingDeadlineSeconds - странная штука. Рекомендуется ставить в половину времени запуска. Контроллер считает сколько пропущенных либо незапущенных попыток стартовать задание за значение этого параметра в секундах, если больше чем половина - отменяет задачу и не будет дальше пытаться запустить джобу. 
  - concurrencyPolicy - Allow (разрешать одновременный запуск джобов - не рекомендуется), Forbid (запретить), Replace (замещать)
  - successfulJobsHistoryLimit - лимит истории удачных джобов
  - failedJobsHistoryLimit - лимит истории неудачных джобов
  - создает объект Job примерно в тот момент, когда он должен быть выполнен. Иногда создается 2 экземпляра джобы, либо ни одного. Джобы должны быть идемпотентны - мочь запускаться одновременно, на разных наборах данных и приводить целевой объект к нужному значению.

### Role-based Access Control (RBAC)

Авторизация
Любой запрос в API - проходит через ролевую модель. Может конкретный пользователь получить тот или иной доступ к объекту API или нет.
K8S ничего не знает о пользователях, как например AD, LDAP, СУБД. Надо через Oauth модуль подключать сервис аутентификации и локальный объект сопоставить с пользователем. Максимум что можно приравнять к пользователю - это ServiceAccount. 

  - Role - список объектов (resources в apiGroup) и прав на них (verbs). Область - конкретный Namespace
  - RoleBinding - связующее звено между сервис-аккаунтом и ролью - сервис аккаунт такой-то с именем таким-то в Namespace таком-то.
  - ClusterRole - то же что и роль, но область - весь кластер. Если указать в RoleBinding кластерную роль, мы берем права на объекты кластерной роли, но только к тем объектам, которые есть в локальном Namespace
  - ClusterRoleBinding - то же что и RoleBinding но для кластерных ролей
  - ServiceAccount - нужен для запуска сервисов, читай подов. По умолчанию создается на каждый namespace и все поды в данном NS работают под данным сервисаккаунтом. Имеется в виду: внутрь контейнера монтируется токен от сервис-аккаунта. 
  Если в HTTP Header указать bearer authorisation и указать в нем токен от сервис аккаунта - можно авторизоваться.
  И приложение может слать с этим токеном запросы в Kube API.

По умолчанию в каждом поде добавляется VolumeMount "default-token-...", куда монтируется токен сервис-аккаунта. 
ApiGroups - концепция для группировки объектов для назначения прав. Объекты, которые создавались в K8S до того как были придуманы apiGroups - лежат в корневой группе - "".
Пример:

`rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - secrets
  - endpoints
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - get
  - list
  - update
  - watch`

Resources - объекты кластеры.
Verbs - права. 

`GET /apis/networking.k8s.io/v1beta1/namespaecs/{namespace}/ingresses/{name}
- apiGroups: [...]
  resources: [...]
  verbs: [...]`

Обычно есть пользователи, и на них навешиваются права. В K8S наоборот - сначала создаются роли и права, а потом мапятся с помощью Binding-ов на пользователей.

Аутентификация по сертификатам: CN берет в качестве kind:User, Organization в качестве kind:Group

Техобслуживание узла.
  - Раньше была опция cordon, узел надо было кордонить (выгонять все поды) перед обслуживанием.
  - теперь на узел вешается Taint:NoSchedule, Taint:NoExecute. Второй Taint видит kubelet и начинает эвиктить поды с узла. Раньше Taint-ы в таком формате были неподдерживаемой опцией.

Что делать если хочется положить БД в K8S?
  - без привязки к куберу, я лично пробовал lustre, gpfs, glusterfs, nfs и чуток ceph, но очень давно.
  - больше всего gpfs зашёл, но очень много бабла надо на нормальный сетап
  - gluster тормозит даже в простейшем сетапе аля сделай две ноды с реплицированием
  - minio не катит если много мелких файлов писать, типа картиночки хранить 
  - для бд - локальный nmve, если много денег - emc unity, для его есть нативный провижинер в кубе
  - если немного денег - drbd(piraeus\linstore), но есть сложности в настройке, большая избыточность.


## Тема №5: DNS в кластере. Публикация сервисов и приложений

Первый запрос от пода - в Local DNS Cache.
Если нет ответа - запрос идет в CoreDNS - основной DNS-сервис в K8S. CoreDNS отдает ответ в кэширующий DNS-сервер и тот отдает ответ поду.
Почему такая схема - потому что в Linux NAT теряются пакеты. Одно из решений - IPVS вместо IPTables, но это плохо. Второй путь - запустить кэширующий DNS-сервер на каждой ноде в виде DaemonSet. Уменьшается поток трафика на основной DNS-сервер, поскольку доступ к нему идет на объект Service, а значит будет работать NAT. 
Запрос в CoreDNS отправляется по TCP, а не по UDP 
DNS-Autoscaler - это специальный под, который будет плодить поды с CoreDNS, в зависимости от размера кластера.

`# cat /etc/resolv.conf` - из пода:
options ndots:5 - внешний запрос будет резолвиться только если у него от 5 точек в имени.

Настройка CoreDNS kubernetes autopath - чтобы внешние запросы сразу отправлялись на внешний DNS.
        kubernetes {
            pods verified
            fallthrough
        }
        autopath @kubernetes

Публикация приложений:
  - через Service: L3 OSI, NAT, kube-proxy
  - через Ingress; L7 OSI, HTTP/HTTPS, nginx/envoy/haproxy/traefik

Типы сервисов:
  - ClusterIP - балансирует трафик на поды. При создании сервиса создается запись в CoreDNS с именем названия сервиса. Нужно в первую очередь для коммуникации внутри кластера. Можно с локальной машины сделать порт-форвард для проверок: `kubectl port-forward service/my-service 10000:80` 
  - NodePort - для доступа снаружи. Открывает порт с большим номером на каждой ноде кластера. 
  - LoadBalancer - для доступа снаружи через балансировщик. Заточено под облачные провайдеры, где можно заказать отдельный контроллер доступа. Технически на baremetal можно попробовать сделать такую публикацию через MetalLB. Есть возможность задать конкретный IP для балансера. 
  - ExternalName - минималистичный сервис, сопоставляем имени сервиса внешней DNS-записи. Для того чтобы поды обращались к сервису изнутри кластера и попадали на сервис снаружи кластера. 
  - ExternalIP - похож на NodePort, но создает правила трансляции для IP-адресов. Обычно это для кейса с VRRP и общим VIP-адресом, этот VIP-адрес вешается на ExternalIP.

Headless Service (безголовый сервис)
  - .spec.clusterIP: None
  - резолвится в IP всех эндпойнтов
  - создает записи с именами всех эндпойнтов
  - если сделать nslookup <servicename.default.svc.clustername.local> - то отдаст стоьлко хостнеймов, сколько подов сидит за сервисом. 

Ingress Controller
  - x.y.z (конкретный домен) - на один сервис
  - y.z/w (sub-path) - на другой сервис
  - others - на третий сервис
  - есть специальные аннотации metadata:annotations:nginx.ingress.kubernetes.io/... - читать в документации. В аннотациях задается сразу кусок конфига nginx, но то что пишется в аннотациях - не проверяется в Kube API на логику, можно легко сломать ингресс. 
  В последней версии ингресс контроллера можно добавить через Helm Chart - Validation Webhook, через который можно валидировать конфиг ингресса до применения.
  - по умолчанию ингресс - HTTP. Для того чтобы сделать HTTPS - надо создать секрет с сертификатом и указать сертификат в Ingress в spec:tls:-hosts:-hostname:..
  `kubectl create secret tls ${CERT_NAME} --key ${KEY_FILE} --cert ${CERT_FILE}`  


## Тема №6: Работа с Helm

Как в принципе деплоить приложения в кластер?
  - последовательно по манифестам - самое простое. Удобно когда мало манифестов и 1-2 приложение
  - темплейтировать приложения:
  Kubectl-based (сгенерить манифесты и отправить в kubectl apply):
    - sed/envsubst
    - ansible - Jinja + подключить плагины для Kubernetes.
    - ksonnet/jsonnet
    - Kapitan (замена Helm)
  Если что-то пошло не так - надо делать kubectl rollout.
  Не-kubectl-based
    - Helm - рендерит манифесты, сохраняет в артефакты (2 версия в ConfigMap, 3 версия в Secret). Версионирование также в секретах, если надо откатиться - берется предыдущий секрет. 
    Это "пакетный менеджер" - разруливает зависимости.
    CNCF - одобренно федерацией Cloud Native
    Декларативный
    Состоит из Heml+Tiller (v2), Tiller - серверная часть, ставился с большими привилегиями, в v3 Tiller выпилили.
    Есть важные фичи для построения CI/CD:
      - watch
      - rollback
    Система плагинов: например Helm-monitor, который может делать проверку выкатку релиза.

Пакет:
  - набор темплейтированных манифестов
  - файл со значением переменных
  - метаданные
  - все упаковывается в .tgz и обзывается чартом

Команды:
  - helm search – поиск чарта
  - helm install – установка чарта
  - helm upgrade – обновление чарта
  - helm get – скачать чарт
  - helm show – показать инфу о чарте
  - helm list – список установленных чартов
  - helm uninstall – удалить чарт

Тестирование релизов при помощи Helm:
  1. Создаем папку templates/tests/
  2. Кладем туда манифесты объектов k8s которые будут тестить релиз
  3. Манифесты должны содержать аннотацию helm.sh/hook: test
  4. Запускаем в CI helm test <release name>

Хуки в Helm
  - pre-install, post-install, pre-delete, post-delete, pre-upgrade, post-upgrade,
pre-rollback, post-rollback
  - Это те же манифесты k8s
  - Одинаковые хуки сортируются по весу и имени объекта
  - Сперва отрабатывают объекты с меньшим весом (от - к +)
  - Хуки не входят в релиз (helm.sh/hook-delete-policy)

Свой репозиторий Helm
  - можно хранить в гите
  - можно сделать хоть на Nginx - должен быть HTTP сервер, умеющий хранить YAML и TAR, где в нужной структуре лежат чарты, архивы и контрольные суммы. 
  - в корневой директории должен быть файл index.yml с индексом всех чартов

Разные полезные команды Helm:
  - Поиск чартов
  `helm search hub`
  - Получение дефолтных values
  `helm show values repo/chart > values.yaml`
  - Установка чарта в кластер
  `helm install release-name repo/chart [--atomic] [--namespace namespace]`
  - Локально отрендерить чарт
  `helm template /path/to/chart`

Холиварные вопросы:
  - где лучше хранить чарты Helm? Вместе с кодом или отдельно?
  - как версионировать чарты?
  - если туча микросервисов - не лучше ли чарты вынести в другую репу?

Helm Linter - проверяет на синтаксис и на bad practices. 
  - команда `helm lint`
  - 

## Тема №7: Ceph, установка в режиме делай как я. Работа с постоянными томами, sc, pvc, pv, подключение томов к подам. Разбор на примере Ceph



## Тема №8: Установка cert-manager



## Тема №9: Обслуживание кластера



## Тема №10: Практическая работа, докеризация приложения и запуск в кластере

